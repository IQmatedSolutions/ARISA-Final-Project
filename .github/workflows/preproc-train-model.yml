name: Download Dataset and Preprocess

on:
  workflow_dispatch:

jobs:
  Preproc:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout repository # Checkout repository to have the latest version of code to use
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1

      - name: Create dataset directory # Create directory where downloaded dataset will be saved
        run: |
          mkdir -p $HOME/dataset

      - name: Download file from S3
        run: aws s3 cp s3://arisa-ml-datasets/ARISA_dataset/ ./dataset --recursive

      - name: Set up Python # Step used to setup proper Python version for code execution
        uses: actions/setup-python@v4
        with: 
          python-version: "3.9"

      - name: Install dependencies # Step used to install all necessary Python libraries based on setup in Makefile
        run: pip install pyyaml  

      - name: Run preprocessing # Execute DSML.preproc.py code to make all necessary data manipulations
        run: make preprocess

      - name: Upload preprocessed data # Attach the preprocessed data as artifact to run
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: ./dataset/processed

  Train:
      runs-on: gpu-runner
      needs: Preproc
      permissions:
        contents: write  # This gives the token write access to the repository contents
      steps:
        - name: Checkout repository # Checkout repository to have the latest version of code to use
          uses: actions/checkout@v4

        - name: Download processed data # Get data preprocessed in previous job to use in predictions
          uses: actions/download-artifact@v4
          with:
            name: processed-data
            path: dataset/processed

        - name: Create dataset directory # Create directory where downloaded dataset will be saved
          run: |
            mkdir -p $HOME/models/my_model

        - name: Set up Python # Step used to setup proper Python version for code execution
          uses: actions/setup-python@v4
          with: 
            python-version: "3.9"

        - name: Install dependencies # Step used to install all necessary Python libraries based on setup in Makefile
          run: pip install ultralytics mlflow

        - name: Run training # Execute DSML.train.py code to make all necessary data manipulations
          env:
            MLFLOW_TRACKING_URI: ${{secrets.MLFLOW_TRACKING_URI}}
            AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
            AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          run: make train

        - name: Commit and push model
          uses: devops-infra/action-commit-push@v0.11
          with:
            github_token: ${{ secrets.GH_TOKEN }}
            commit_prefix: "[AUTO-COMMIT] "
            commit_message: "Update model version"
            add: models/my_model/
