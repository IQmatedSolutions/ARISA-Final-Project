name: Download Dataset, Preprocess and Train Model

on:
  workflow_dispatch:

jobs:
  Preproc:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout repository # Checkout repository to have the latest version of code to use
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1

      - name: Create dataset directory # Create directory where downloaded dataset will be saved
        run: |
          mkdir -p $HOME/dataset

      - name: Download file from S3
        run: aws s3 cp s3://arisa-ml-datasets/ARISA_dataset/ ./dataset --recursive

      - name: Set up Python # Step used to setup proper Python version for code execution
        uses: actions/setup-python@v4
        with: 
          python-version: "3.9"

      - name: Install dependencies # Step used to install all necessary Python libraries based on setup in Makefile
        run: pip install pyyaml  

      - name: Run preprocessing # Execute DSML.preproc.py code to make all necessary data manipulations
        run: make preprocess

      - name: Upload preprocessed data # Attach the preprocessed data as artifact to run
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: ./dataset/processed

  Train:
      runs-on: gpu-runner
      needs: Preproc
      permissions:
        contents: write  # This gives the token write access to the repository contents
        id-token: write
      steps:
        - name: Checkout repository # Checkout repository to have the latest version of code to use
          uses: actions/checkout@v4

        - name: Download processed data # Get data preprocessed in previous job to use in predictions
          uses: actions/download-artifact@v4
          with:
            name: processed-data
            path: dataset/processed

        - name: Create dataset directory # Create directory where downloaded dataset will be saved
          run: |
            mkdir -p models/my_model

        - name: Set up Python # Step used to setup proper Python version for code execution
          uses: actions/setup-python@v4
          with: 
            python-version: "3.9"

        - name: Install dependencies # Step used to install all necessary Python libraries based on setup in Makefile
          run: pip install ultralytics mlflow

        - name: Run training # Execute DSML.train.py code to make all necessary data manipulations
          env:
            MLFLOW_TRACKING_URI: ${{secrets.MLFLOW_TRACKING_URI}}
            AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
            AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          run: make train

        - name: Export model to NCNN # Execute DSML.train.py code to make all necessary data manipulations
          run: | 
            cp /home/runner/work/_tool/Python/3.9.23/x64/lib/python3.9/site-packages/tests/tmp/runs/detect/train/weights/best.pt models/my_model/ARISA_new.pt
            yolo export model=models/my_model/ARISA_new.pt format=ncnn

        - name: Set up Git
          run: |
            git config --global user.name "GitHub Actions Bot"
            git config --global user.email "github-actions-bot@users.noreply.github.com"

        - name: Commit and push changes
          run: |
            git add models/my_model/
            git commit -m "Update model version"
            git push

        - name: Trigger Deployment Workflow # Execute deployment
          run: |
            curl -X POST -H "Authorization: Bearer ${{ secrets.GH_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            https://api.github.com/repos/${{ github.repository }}/actions/workflows/deploy-model.yml/dispatches \
            -d '{"ref":"main"}'